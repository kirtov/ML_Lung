{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from scipy.io.wavfile import read as wavread\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, confusion_matrix, matthews_corrcoef\n",
    "from sklearn.cross_validation import train_test_split, KFold, StratifiedKFold\n",
    "import pandas as pd\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.signal import argrelextrema\n",
    "import seaborn\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from sklearn.svm import LinearSVC\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#DATA PATHS\n",
    "sick_path = \"./data/LungDiagnostics-sorted/_sick/\"\n",
    "nonsick_path = \"./data/LungDiagnostics-sorted/_not-sick/\"\n",
    "val_path = \"./data/LungDiagnostics-sorted/new_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#PREPROCESS\n",
    "def ffilter(X):\n",
    "    new_X = []\n",
    "    step = 8000\n",
    "    for i in range(0, len(X), step):\n",
    "        new_X = new_X + [np.mean(X[i:min(i+step, len(X))])]*step\n",
    "    return np.array(new_X)\n",
    "\n",
    "def get_data(path):\n",
    "    d = wavread(path)\n",
    "    rate = d[0]\n",
    "    sound = d[1]\n",
    "    return sound, rate\n",
    "\n",
    "def smooth(sound_copy, quantile = 40):\n",
    "    sound_copy[sound_copy < 0] = 0\n",
    "    sound_copy_smoothed = ffilter(sound_copy)\n",
    "    sound_copy_smoothed[sound_copy_smoothed < np.percentile(sound_copy_smoothed, quantile)] = 0\n",
    "    return sound_copy_smoothed\n",
    "    \n",
    "def find_biggest_peaks(sound_copy_smoothed, count = 2):\n",
    "    dots_begin = []\n",
    "    dots_end = []\n",
    "    for i in range(1, len(sound_copy_smoothed)-1):\n",
    "        if (sound_copy_smoothed[i] == 0 and sound_copy_smoothed[i + 1] > 0):\n",
    "            dots_begin.append(i)\n",
    "        if (sound_copy_smoothed[i] > 0 and sound_copy_smoothed[i + 1] == 0):\n",
    "            if (len(dots_begin) == len(dots_end)):\n",
    "                dots_begin.append(0)\n",
    "            dots_end.append(i)\n",
    "    if (len(dots_begin) > len(dots_end)):\n",
    "        dots_end.append(len(sound_copy_smoothed))\n",
    "    dots_begin = np.array(dots_begin)\n",
    "    dots_end = np.array(dots_end)\n",
    "    peak_lens = dots_end - dots_begin\n",
    "    biggest_peaks = np.argsort(peak_lens)[::-1][:2]\n",
    "    if (len(biggest_peaks) < 2):\n",
    "        return None\n",
    "    else:\n",
    "        result = []\n",
    "        for i in biggest_peaks:\n",
    "            result.append([dots_begin[i], dots_end[i]])\n",
    "        return result\n",
    "    \n",
    "def extract_respiratory_cycle(path):\n",
    "    data = []\n",
    "    target = []\n",
    "    rates = []\n",
    "    errors = 0\n",
    "    for f in os.listdir(path):\n",
    "        if ('wav' not in f):\n",
    "            continue\n",
    "        if (('not' in f) or ('non' in f)):\n",
    "            label = 0\n",
    "        else:\n",
    "            label = 1\n",
    "        #Read sound data and rate\n",
    "        sound, rate = get_data(path + f)\n",
    "        sound_copy = deepcopy(sound)\n",
    "        #Smooth sound\n",
    "        sound_copy_smoothed = smooth(sound_copy, 40)\n",
    "        #Find peaks\n",
    "        dots = find_biggest_peaks(sound_copy_smoothed)\n",
    "        if (dots == None):\n",
    "            errors += 1\n",
    "        else:\n",
    "            sample = np.concatenate([sound[d[0]:d[1]] for d in dots])\n",
    "            data.append(sample)\n",
    "            rates.append(rate)\n",
    "            target.append(label)\n",
    "    print(\"Errors count {}\".format(errors))\n",
    "    return np.array(data), np.array(target), np.array(rates)\n",
    "\n",
    "# data_val, target_val, rates_val = extract_respiratory_cycle(val_path)\n",
    "# data_sick, target_sick, rates_sick = extract_respiratory_cycle(sick_path)\n",
    "# data_nsick, target_nsick, rates_nsick = extract_respiratory_cycle(nonsick_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_val, target_val, rates_val = pickle.load(open(\"./data/preprocessed/val_data.pkl\", 'rb'))\n",
    "data_sick, target_sick, rates_sick = pickle.load(open(\"./data/preprocessed/sick_data.pkl\", 'rb'))\n",
    "data_nsick, target_nsick, rates_nsick = pickle.load(open(\"./data/preprocessed/nsick_data.pkl\", 'rb'))\n",
    "\n",
    "data_train = np.concatenate([data_sick, data_nsick])\n",
    "target_train = np.concatenate([target_sick, target_nsick])\n",
    "rates_train = np.concatenate([rates_sick, rates_nsick])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#FEATURE EXTRACTION\n",
    "from python_speech_features import mfcc\n",
    "from python_speech_features import delta\n",
    "from python_speech_features import logfbank\n",
    "from PIL import Image\n",
    "import PIL\n",
    "\n",
    "mfcc_train = []\n",
    "mfcc_val = []\n",
    "numcep = 13\n",
    "height = 350\n",
    "for d,r in zip(data_train, rates_train):\n",
    "    (rate,sig) = r, d\n",
    "    mfcc_feat = mfcc(sig,rate, numcep=numcep)\n",
    "    d_mfcc_feat = delta(mfcc_feat, 2)\n",
    "    d_mfcc_zip = np.array(Image.fromarray(d_mfcc_feat).resize((numcep, height), PIL.Image.NEAREST))\n",
    "    mfcc_train.append(d_mfcc_zip)\n",
    "mfcc_train = np.array(mfcc_train)\n",
    "\n",
    "for d,r in zip(data_val, rates_val):\n",
    "    (rate,sig) = r, d\n",
    "    mfcc_feat = mfcc(sig,rate, numcep=numcep)\n",
    "    d_mfcc_feat = delta(mfcc_feat, 2)\n",
    "    d_mfcc_zip = np.array(Image.fromarray(d_mfcc_feat).resize((numcep, height), PIL.Image.NEAREST))\n",
    "    mfcc_val.append(d_mfcc_zip)\n",
    "mfcc_val = np.array(mfcc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mfcc_train_flatten = np.array([x.flatten() for x in mfcc_train])\n",
    "mfcc_val_flatten = np.array([x.flatten() for x in mfcc_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "Test Accuracy: 0.817\n",
      "Test F1: 0.884\n",
      "Test MTTC: 0.482\n",
      "Test CM: [[14  5]\n",
      " [16 80]]\n",
      "Validation Accuracy: 0.714\n",
      "Validation F1: 0.8\n",
      "Validation MTTC: 0.471\n",
      "Validation CM: [[ 3  6]\n",
      " [ 0 12]]\n",
      "---------------\n",
      "RandomForest\n",
      "Test Accuracy: 0.957\n",
      "Test F1: 0.975\n",
      "Test MTTC: 0.837\n",
      "Test CM: [[14  5]\n",
      " [ 0 96]]\n",
      "Validation Accuracy: 0.571\n",
      "Validation F1: 0.727\n",
      "Validation MTTC: 0.0\n",
      "Validation CM: [[ 0  9]\n",
      " [ 0 12]]\n",
      "---------------\n",
      "GBM\n",
      "Test Accuracy: 0.957\n",
      "Test F1: 0.975\n",
      "Test MTTC: 0.837\n",
      "Test CM: [[14  5]\n",
      " [ 0 96]]\n",
      "Validation Accuracy: 0.571\n",
      "Validation F1: 0.727\n",
      "Validation MTTC: 0.0\n",
      "Validation CM: [[ 0  9]\n",
      " [ 0 12]]\n",
      "---------------\n",
      "SVM\n",
      "Test Accuracy: 0.826\n",
      "Test F1: 0.89\n",
      "Test MTTC: 0.497\n",
      "Test CM: [[14  5]\n",
      " [15 81]]\n",
      "Validation Accuracy: 0.714\n",
      "Validation F1: 0.8\n",
      "Validation MTTC: 0.471\n",
      "Validation CM: [[ 3  6]\n",
      " [ 0 12]]\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "#MODEL EVALUATION\n",
    "def get_model_by_name(model_name):\n",
    "    models = {\"LogisticRegression\" : LogisticRegression(), 'RandomForest' : RandomForestClassifier(n_jobs=4, n_estimators=100), \n",
    "           'GBM' : GradientBoostingClassifier(), 'SVM': LinearSVC()}\n",
    "    if (model_name in models):\n",
    "        return models[model_name]\n",
    "    else:\n",
    "        print(\"Illegal model name\")\n",
    "        \n",
    "def calc_metrics(true, pred, pref):\n",
    "    acc = accuracy_score(true, pred.round())\n",
    "    print(\"{} Accuracy: {}\".format(pref, np.round(acc,3)))\n",
    "    f1 = f1_score(true, pred.round())\n",
    "    print(\"{} F1: {}\".format(pref, np.round(f1,3)))\n",
    "    mttc = matthews_corrcoef(true, pred.round())\n",
    "    print(\"{} MTTC: {}\".format(pref, np.round(mttc,3)))\n",
    "    cm = confusion_matrix(true, pred.round())\n",
    "    print(\"{} CM: {}\".format(pref, cm))\n",
    "\n",
    "def train_model(X_train, y_train, X_test, y_test, model_name):\n",
    "    model = get_model_by_name(model_name)\n",
    "    model.fit(X_train, y_train)\n",
    "    test_predicts = model.predict(X_test)\n",
    "    calc_metrics(y_test, test_predicts, \"Test\")\n",
    "    return model\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(mfcc_train_flatten, target_train, test_size=0.25, random_state=23)\n",
    "\n",
    "for model_name in ['LogisticRegression', 'RandomForest', 'GBM', 'SVM']:\n",
    "    print(model_name)\n",
    "    model = train_model(X_train, y_train, X_test, y_test, model_name)\n",
    "    val_predicts = model.predict(mfcc_val_flatten)\n",
    "    calc_metrics(target_val, val_predicts, \"Validation\")\n",
    "    print(\"-\"*15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_train = np.reshape(y_train, (y_train.shape[0], 1))\n",
    "y_test = np.reshape(y_test, (y_test.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.548\n",
      "Test F1: 0.662\n",
      "Test MTTC: 0.121\n",
      "Test CM: [[12  7]\n",
      " [45 51]]\n",
      "Validation Accuracy: 0.667\n",
      "Validation F1: 0.774\n",
      "Validation MTTC: 0.375\n",
      "Validation CM: [[ 2  7]\n",
      " [ 0 12]]\n"
     ]
    }
   ],
   "source": [
    "nn = NN(X_train.shape[1])\n",
    "nn.fit(X_train, y_train, X_test, y_test, training_epochs=50)\n",
    "test_predicts = nn.predict(X_test)\n",
    "calc_metrics(y_test, test_predicts, \"Test\")\n",
    "val_predicts = nn.predict(mfcc_val_flatten)\n",
    "calc_metrics(target_val, val_predicts, \"Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class NN(object):       \n",
    "    def __init__(self, n_input, n_hidden_1 = 500, n_hidden_2 = 500, n_classes = 1):\n",
    "        self.display_step = 1\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden_1 = n_hidden_1\n",
    "        self.n_hidden_2 = n_hidden_2\n",
    "        self.n_classes = n_classes\n",
    "        # Store layers weight & bias\n",
    "        weights = {\n",
    "            'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "            'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "            'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "        }\n",
    "        biases = {\n",
    "            'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "            'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "            'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "        }\n",
    "        # tf Graph input\n",
    "        self.x = tf.placeholder(\"float\", [None, self.n_input])\n",
    "        self.y = tf.placeholder(\"float\", [None, self.n_classes])\n",
    "        # Construct model\n",
    "        self.pred = self.multilayer_perceptron(self.x, weights, biases)\n",
    "     \n",
    "    def multilayer_perceptron(self, x, weights, biases):\n",
    "        # Hidden layer with RELU activation\n",
    "        layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "        layer_1 = tf.nn.relu(layer_1)\n",
    "        # Hidden layer with RELU activation\n",
    "        layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "        layer_2 = tf.nn.relu(layer_2)\n",
    "        # Output layer with linear activation\n",
    "        out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "        out_layer = tf.nn.sigmoid(out_layer)\n",
    "        return out_layer\n",
    "\n",
    "    def fit(self, X_train = None, y_train = None, X_test = None, y_test = None, training_epochs = 10, batch_size = 16, display_step = 1,learning_rate = 0.01):\n",
    "        # Define loss and optimizer\n",
    "        cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.pred, labels=self.y))\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "        saver = tf.train.Saver()\n",
    "        # Initializing the variables\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        # Launch the graph\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(self.init)\n",
    "            # Training cycle\n",
    "            for epoch in range(training_epochs):\n",
    "                total_batch = int(len(X_train)/batch_size)\n",
    "                # Loop over all batches\n",
    "                for i in range(total_batch):\n",
    "                    batch_x = X_train[i*batch_size:(i+1)*batch_size]\n",
    "                    batch_y = y_train[i*batch_size:(i+1)*batch_size]\n",
    "                    sess.run([optimizer, cost], feed_dict={self.x: batch_x,\n",
    "                                                                  self.y: batch_y})\n",
    "            \n",
    "    def predict(self, X_test=None):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(self.init)\n",
    "            pr = sess.run(self.pred, feed_dict={self.x: X_test})\n",
    "        return pr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
